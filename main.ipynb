{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_protein_file(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        # Read the entire contents of the file\n",
    "        seq = \"\"\n",
    "        str = \"\"\n",
    "\n",
    "        sequences = []\n",
    "        strings = []\n",
    "        count = 0\n",
    "        for line in file:\n",
    "            if count < 8:\n",
    "                pass\n",
    "            count += 1\n",
    "\n",
    "            line = line.strip()\n",
    "            if line == \"<>\":\n",
    "                seq = \"\"\n",
    "                str = \"\"\n",
    "            elif line == \"<end>\" or line==\"end\":\n",
    "                sequences.append(seq)\n",
    "                strings.append(str)\n",
    "            else:\n",
    "                letters = line.split(\" \")\n",
    "                if len(letters) == 2:\n",
    "                    seq += letters[0]\n",
    "                    str += letters[1] if letters[1] != \"_\" else \"c\"\n",
    "        return sequences, strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq, train_str = read_protein_file(\"data/protein-secondary-structure.train\")\n",
    "test_seq, test_str = read_protein_file(\"data/protein-secondary-structure.test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE TO SELF\n",
    "Create sliding windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note. we need to split the sequences up, but since our test input is of different lengths, what do we do then?\n",
    "\n",
    "When your test data contains protein sequences of different lengths compared to your training data, you need to handle this discrepancy appropriately. Here are a few strategies you can consider:\n",
    "\n",
    "Padding:\n",
    "\n",
    "Pad shorter sequences in the test data to match the length of the longest sequence in your training data.\n",
    "You can pad the sequences with a special token (e.g., all zeros) so that the neural network can recognize them as padding.\n",
    "During inference, you would need to trim the predictions to the original length of the test sequence.\n",
    "Dynamic Input Shape:\n",
    "\n",
    "Modify your neural network architecture to accept variable-length input sequences.\n",
    "Use techniques like masking to handle variable-length sequences effectively.\n",
    "Many deep learning frameworks, such as TensorFlow and PyTorch, support dynamic input shapes and masking.\n",
    "Bucketing or Binning:\n",
    "\n",
    "Group sequences of similar lengths together and pad each group separately to the maximum length within that group.\n",
    "This approach reduces the amount of padding required, leading to more efficient training.\n",
    "You can organize your test data into buckets or bins based on sequence lengths and process each bucket separately during evaluation.\n",
    "Batch Processing:\n",
    "\n",
    "While processing batches during testing, group sequences of similar lengths together.\n",
    "This allows you to minimize padding within each batch, improving computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding the sequence\n",
    "\n",
    "Encoding Amino Acids:\n",
    "\n",
    "First, you need to encode each amino acid into a numerical representation.\n",
    "Common encoding methods include one-hot encoding and embedding.\n",
    "In one-hot encoding, each amino acid is represented as a binary vector where only one element is 1 (indicating the presence of that amino acid).\n",
    "In embedding, each amino acid is mapped to a low-dimensional vector space.\n",
    "Padding:\n",
    "\n",
    "After encoding amino acids, you can proceed with padding sequences as discussed earlier.\n",
    "Pad the sequences to match the length of the longest sequence in your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERFORM NEURAL NETWORK??? use RNN or CNN (keras, tensorflow, pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.pnas.org/doi/epdf/10.1073/pnas.86.1.152\n",
    "\n",
    "sliding window = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_based_on_windows(data_seq, data_str, W=17):\n",
    "    all_sequences = []\n",
    "    all_strings = []\n",
    "    for i in range(len(data_seq)):\n",
    "        sequences = [data_seq[i][j:j+W] for j in range(0, len(data_seq[i]), W)]\n",
    "        strings = [data_str[i][j:j+W] for j in range(0, len(data_str[i]), W)]\n",
    "        # PADDING\n",
    "        if len(sequences[-1]) != W:\n",
    "            sequences[-1] = sequences[-1] + \"X\"*((W-len(sequences[-1])))\n",
    "        if len(strings[-1]) != W:\n",
    "            strings[-1] = strings[-1] + \"X\"*((W-len(strings[-1])))\n",
    "\n",
    "        all_sequences += sequences\n",
    "        all_strings += strings\n",
    "\n",
    "    train_df = pd.DataFrame({\"sequence\": all_sequences, \"string\": all_strings})\n",
    "    return train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = split_based_on_windows(train_seq, train_str)\n",
    "test_df = split_based_on_windows(test_seq, test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_from_letter_cols(letters, df):\n",
    "    all_columns = []\n",
    "    for i in letters:\n",
    "        columns = [str(j)+\"_\"+i for j in range(0, 17)]\n",
    "        all_columns += columns\n",
    "\n",
    "    template = pd.DataFrame(columns=all_columns, dtype=bool)\n",
    "    unique_columns = template.columns.difference(df.columns)\n",
    "\n",
    "    one_hot_encoded_full = pd.concat([df, template[unique_columns]], axis=1).fillna(False)\n",
    "    ohe_df = one_hot_encoded_full.reindex(sorted(one_hot_encoded_full.columns), axis=1).astype(int)\n",
    "    return ohe_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ohe_matrix(df):\n",
    "    X_letters = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"X\", \"Y\"]\n",
    "    Y_letters = ['c', 'e', 'h', 'X']\n",
    "\n",
    "    by_pos_df = df['sequence'].apply(lambda x:pd.Series(list(x)))\n",
    "    ohe_df_X = pd.get_dummies(by_pos_df)\n",
    "    ohe_X = get_matrix_from_letter_cols(X_letters, ohe_df_X)\n",
    "\n",
    "    by_pos_df = df['string'].apply(lambda x:pd.Series(list(x)))\n",
    "    ohe_df_Y = pd.get_dummies(by_pos_df)\n",
    "    ohe_Y = get_matrix_from_letter_cols(Y_letters, ohe_df_Y)\n",
    "\n",
    "    return ohe_X, ohe_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ohe_X, train_ohe_Y = get_ohe_matrix(train_df)\n",
    "test_ohe_X, test_ohe_Y = get_ohe_matrix(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Apply thresholding to the predicted values\n",
    "    y_pred_binary = tf.cast(tf.greater_equal(y_pred, 0.5), tf.float32)\n",
    "    \n",
    "    # Binary cross-entropy loss for binary classification\n",
    "    bce_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Penalty term for deviation from 17 ones\n",
    "    ones_penalty = tf.reduce_sum(tf.abs(tf.reduce_sum(y_pred_binary, axis=1) - 17))  # Compute the absolute difference from 17 ones\n",
    "    \n",
    "    # Combine binary cross-entropy and penalty term\n",
    "    total_loss = bce_loss + 1 * ones_penalty  # Adjust the penalty coefficient as needed\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom constraint for the output layer\n",
    "class OnesConstraint(tf.keras.constraints.Constraint):\n",
    "    def __init__(self, target_ones):\n",
    "        self.target_ones = target_ones\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return w\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'target_ones': self.target_ones}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 [==============================] - 1s 10ms/step - loss: 218.9177 - accuracy: 0.0535 - val_loss: 71.9997 - val_accuracy: 0.1658\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 141.1890 - accuracy: 0.2610 - val_loss: 165.0746 - val_accuracy: 0.3422\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 158.6181 - accuracy: 0.2503 - val_loss: 141.9197 - val_accuracy: 0.2299\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 165.8090 - accuracy: 0.1673 - val_loss: 130.2999 - val_accuracy: 0.1497\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 117.7611 - accuracy: 0.1191 - val_loss: 132.3707 - val_accuracy: 0.1337\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 145.9896 - accuracy: 0.1124 - val_loss: 127.8382 - val_accuracy: 0.1070\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 164.8151 - accuracy: 0.1526 - val_loss: 130.5850 - val_accuracy: 0.1016\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 145.1615 - accuracy: 0.0937 - val_loss: 139.6224 - val_accuracy: 0.0695\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 149.9529 - accuracy: 0.0589 - val_loss: 153.1475 - val_accuracy: 0.0642\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 149.2135 - accuracy: 0.0924 - val_loss: 153.2518 - val_accuracy: 0.0909\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 143.7232 - accuracy: 0.0482 - val_loss: 158.1963 - val_accuracy: 0.0481\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 135.7271 - accuracy: 0.0402 - val_loss: 157.8900 - val_accuracy: 0.0481\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 130.5119 - accuracy: 0.0361 - val_loss: 159.2855 - val_accuracy: 0.0428\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 124.7288 - accuracy: 0.0402 - val_loss: 154.4836 - val_accuracy: 0.0535\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 121.9758 - accuracy: 0.0308 - val_loss: 143.9716 - val_accuracy: 0.0481\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 113.6025 - accuracy: 0.0361 - val_loss: 148.6034 - val_accuracy: 0.0481\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 114.9745 - accuracy: 0.0268 - val_loss: 139.2343 - val_accuracy: 0.0428\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 102.9074 - accuracy: 0.0375 - val_loss: 145.0932 - val_accuracy: 0.0374\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 105.2738 - accuracy: 0.0335 - val_loss: 132.4240 - val_accuracy: 0.0374\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 96.3234 - accuracy: 0.0335 - val_loss: 138.8289 - val_accuracy: 0.0374\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 97.3515 - accuracy: 0.0321 - val_loss: 129.9232 - val_accuracy: 0.0374\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 94.8454 - accuracy: 0.0348 - val_loss: 127.7650 - val_accuracy: 0.0374\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 90.7116 - accuracy: 0.0375 - val_loss: 132.5430 - val_accuracy: 0.0321\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 87.1018 - accuracy: 0.0281 - val_loss: 125.9524 - val_accuracy: 0.0321\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 84.9331 - accuracy: 0.0321 - val_loss: 120.7854 - val_accuracy: 0.0321\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 83.2385 - accuracy: 0.0281 - val_loss: 118.6725 - val_accuracy: 0.0321\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 77.9948 - accuracy: 0.0295 - val_loss: 121.3847 - val_accuracy: 0.0321\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 76.3031 - accuracy: 0.0361 - val_loss: 121.8221 - val_accuracy: 0.0321\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 76.1267 - accuracy: 0.0335 - val_loss: 116.5737 - val_accuracy: 0.0321\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 71.9348 - accuracy: 0.0281 - val_loss: 119.8278 - val_accuracy: 0.0374\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 71.1122 - accuracy: 0.0348 - val_loss: 119.5274 - val_accuracy: 0.0374\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 69.4696 - accuracy: 0.0335 - val_loss: 116.0712 - val_accuracy: 0.0374\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 67.2339 - accuracy: 0.0321 - val_loss: 118.6954 - val_accuracy: 0.0374\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 65.5269 - accuracy: 0.0321 - val_loss: 115.6088 - val_accuracy: 0.0374\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 65.2915 - accuracy: 0.0295 - val_loss: 115.0715 - val_accuracy: 0.0374\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 63.6559 - accuracy: 0.0335 - val_loss: 112.3389 - val_accuracy: 0.0374\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 62.1876 - accuracy: 0.0348 - val_loss: 108.8557 - val_accuracy: 0.0374\n",
      "Epoch 38/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 60.0409 - accuracy: 0.0335 - val_loss: 108.6623 - val_accuracy: 0.0374\n",
      "Epoch 39/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 59.7161 - accuracy: 0.0295 - val_loss: 107.8472 - val_accuracy: 0.0374\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 56.2163 - accuracy: 0.0348 - val_loss: 109.4472 - val_accuracy: 0.0321\n",
      "Epoch 41/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 55.8019 - accuracy: 0.0335 - val_loss: 106.1077 - val_accuracy: 0.0374\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 56.1269 - accuracy: 0.0308 - val_loss: 103.4938 - val_accuracy: 0.0374\n",
      "Epoch 43/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 52.8709 - accuracy: 0.0361 - val_loss: 105.6573 - val_accuracy: 0.0374\n",
      "Epoch 44/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 52.9508 - accuracy: 0.0308 - val_loss: 100.1497 - val_accuracy: 0.0374\n",
      "Epoch 45/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 50.8327 - accuracy: 0.0335 - val_loss: 105.8912 - val_accuracy: 0.0374\n",
      "Epoch 46/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 49.8486 - accuracy: 0.0308 - val_loss: 102.7246 - val_accuracy: 0.0374\n",
      "Epoch 47/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 48.7322 - accuracy: 0.0348 - val_loss: 103.6208 - val_accuracy: 0.0374\n",
      "Epoch 48/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 46.9382 - accuracy: 0.0308 - val_loss: 100.8612 - val_accuracy: 0.0321\n",
      "Epoch 49/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 46.1685 - accuracy: 0.0281 - val_loss: 99.1702 - val_accuracy: 0.0321\n",
      "Epoch 50/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 45.4486 - accuracy: 0.0295 - val_loss: 97.7531 - val_accuracy: 0.0321\n",
      "Epoch 51/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 44.8906 - accuracy: 0.0348 - val_loss: 98.0455 - val_accuracy: 0.0321\n",
      "Epoch 52/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 43.1050 - accuracy: 0.0281 - val_loss: 96.6179 - val_accuracy: 0.0321\n",
      "Epoch 53/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 42.9422 - accuracy: 0.0321 - val_loss: 94.2003 - val_accuracy: 0.0321\n",
      "Epoch 54/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 42.0842 - accuracy: 0.0308 - val_loss: 95.7180 - val_accuracy: 0.0321\n",
      "Epoch 55/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 40.1566 - accuracy: 0.0348 - val_loss: 94.1006 - val_accuracy: 0.0321\n",
      "Epoch 56/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 41.0930 - accuracy: 0.0308 - val_loss: 91.1619 - val_accuracy: 0.0321\n",
      "Epoch 57/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 38.5474 - accuracy: 0.0348 - val_loss: 91.5157 - val_accuracy: 0.0321\n",
      "Epoch 58/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 39.2185 - accuracy: 0.0335 - val_loss: 90.2704 - val_accuracy: 0.0374\n",
      "Epoch 59/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 37.8576 - accuracy: 0.0321 - val_loss: 92.6964 - val_accuracy: 0.0374\n",
      "Epoch 60/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 36.2156 - accuracy: 0.0361 - val_loss: 89.8466 - val_accuracy: 0.0321\n",
      "Epoch 61/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 36.4706 - accuracy: 0.0361 - val_loss: 89.2835 - val_accuracy: 0.0374\n",
      "Epoch 62/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 35.3575 - accuracy: 0.0335 - val_loss: 88.3523 - val_accuracy: 0.0321\n",
      "Epoch 63/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 33.3839 - accuracy: 0.0375 - val_loss: 91.4371 - val_accuracy: 0.0374\n",
      "Epoch 64/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 33.2924 - accuracy: 0.0335 - val_loss: 89.4252 - val_accuracy: 0.0428\n",
      "Epoch 65/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 32.9223 - accuracy: 0.0361 - val_loss: 87.9438 - val_accuracy: 0.0428\n",
      "Epoch 66/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 32.8348 - accuracy: 0.0388 - val_loss: 86.7131 - val_accuracy: 0.0428\n",
      "Epoch 67/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 31.4340 - accuracy: 0.0335 - val_loss: 86.7811 - val_accuracy: 0.0374\n",
      "Epoch 68/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 30.4296 - accuracy: 0.0375 - val_loss: 86.9309 - val_accuracy: 0.0428\n",
      "Epoch 69/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 30.9848 - accuracy: 0.0361 - val_loss: 86.5920 - val_accuracy: 0.0428\n",
      "Epoch 70/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 29.7729 - accuracy: 0.0361 - val_loss: 87.6523 - val_accuracy: 0.0374\n",
      "Epoch 71/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 29.3537 - accuracy: 0.0335 - val_loss: 87.0626 - val_accuracy: 0.0428\n",
      "Epoch 72/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 28.5406 - accuracy: 0.0375 - val_loss: 84.3019 - val_accuracy: 0.0428\n",
      "Epoch 73/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 27.5527 - accuracy: 0.0348 - val_loss: 85.1612 - val_accuracy: 0.0428\n",
      "Epoch 74/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 25.8388 - accuracy: 0.0361 - val_loss: 83.8247 - val_accuracy: 0.0374\n",
      "Epoch 75/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 26.0487 - accuracy: 0.0335 - val_loss: 85.2612 - val_accuracy: 0.0428\n",
      "Epoch 76/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 24.8492 - accuracy: 0.0348 - val_loss: 83.5836 - val_accuracy: 0.0374\n",
      "Epoch 77/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 23.9601 - accuracy: 0.0375 - val_loss: 83.6404 - val_accuracy: 0.0428\n",
      "Epoch 78/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 23.4902 - accuracy: 0.0361 - val_loss: 84.3570 - val_accuracy: 0.0374\n",
      "Epoch 79/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 23.1179 - accuracy: 0.0388 - val_loss: 83.8199 - val_accuracy: 0.0428\n",
      "Epoch 80/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 21.8448 - accuracy: 0.0402 - val_loss: 85.3908 - val_accuracy: 0.0428\n",
      "Epoch 81/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 21.4003 - accuracy: 0.0361 - val_loss: 82.8831 - val_accuracy: 0.0374\n",
      "Epoch 82/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 21.2972 - accuracy: 0.0348 - val_loss: 83.3470 - val_accuracy: 0.0374\n",
      "Epoch 83/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.1673 - accuracy: 0.0375 - val_loss: 83.0103 - val_accuracy: 0.0374\n",
      "Epoch 84/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 19.8247 - accuracy: 0.0388 - val_loss: 83.4086 - val_accuracy: 0.0374\n",
      "Epoch 85/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 20.2104 - accuracy: 0.0361 - val_loss: 83.6769 - val_accuracy: 0.0374\n",
      "Epoch 86/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 18.9802 - accuracy: 0.0361 - val_loss: 82.1953 - val_accuracy: 0.0374\n",
      "Epoch 87/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 19.0352 - accuracy: 0.0402 - val_loss: 83.3700 - val_accuracy: 0.0374\n",
      "Epoch 88/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 18.1330 - accuracy: 0.0321 - val_loss: 81.9803 - val_accuracy: 0.0374\n",
      "Epoch 89/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 17.2189 - accuracy: 0.0348 - val_loss: 82.1286 - val_accuracy: 0.0374\n",
      "Epoch 90/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 17.4894 - accuracy: 0.0321 - val_loss: 81.7638 - val_accuracy: 0.0374\n",
      "Epoch 91/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 17.0318 - accuracy: 0.0375 - val_loss: 81.3618 - val_accuracy: 0.0428\n",
      "Epoch 92/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 16.6035 - accuracy: 0.0335 - val_loss: 82.1425 - val_accuracy: 0.0428\n",
      "Epoch 93/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 15.8729 - accuracy: 0.0321 - val_loss: 81.8309 - val_accuracy: 0.0428\n",
      "Epoch 94/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 15.2145 - accuracy: 0.0361 - val_loss: 81.4689 - val_accuracy: 0.0428\n",
      "Epoch 95/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 14.5147 - accuracy: 0.0335 - val_loss: 82.4437 - val_accuracy: 0.0428\n",
      "Epoch 96/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 14.2821 - accuracy: 0.0335 - val_loss: 80.8193 - val_accuracy: 0.0428\n",
      "Epoch 97/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 13.8393 - accuracy: 0.0348 - val_loss: 79.9144 - val_accuracy: 0.0481\n",
      "Epoch 98/100\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 13.3978 - accuracy: 0.0348 - val_loss: 80.9451 - val_accuracy: 0.0481\n",
      "Epoch 99/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 12.6096 - accuracy: 0.0348 - val_loss: 79.9493 - val_accuracy: 0.0428\n",
      "Epoch 100/100\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 12.6663 - accuracy: 0.0335 - val_loss: 80.4138 - val_accuracy: 0.0481\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[446], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_ohe_X, train_ohe_Y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_classes\u001b[49m(test_ohe_X)\n\u001b[0;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(predictions \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     14\u001b[0m output\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(units=train_ohe_Y.shape[1],input_shape=(train_ohe_X.shape[1],), activation=\"relu\"),\n",
    "    layers.Dense(train_ohe_Y.shape[1], activation='sigmoid', kernel_constraint=OnesConstraint(17))\n",
    "\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=custom_loss, optimizer= \"adam\", metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_ohe_X, train_ohe_Y, epochs=100, batch_size=32, validation_split=0.2)\n",
    "predictions = model.predict(test_ohe_X)\n",
    "output = np.where(predictions > 0.5, 1, 0)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the prediction back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 1, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 1, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 68)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mat_back_to_seq(mat):\n",
    "    letters = ['c', 'e', 'h', 'X']\n",
    "\n",
    "    all_columns = []\n",
    "    for i in letters:\n",
    "        columns = [str(j)+\"_\"+i for j in range(0, 17)]\n",
    "        all_columns += columns\n",
    "\n",
    "    template = pd.DataFrame(mat, columns=all_columns, dtype=bool)\n",
    "\n",
    "    strings = []\n",
    "    for i in range(mat.shape[0]):\n",
    "        string = \"\"\n",
    "        for j in range(mat.shape[1]//len(letters)):\n",
    "            for k in letters:\n",
    "                if template[f\"{j}_{k}\"].iloc[i]:\n",
    "                    string+=k\n",
    "                    break\n",
    "        strings.append(string)\n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sequences = convert_mat_back_to_seq(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predict'] = pred_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>string</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENLKLGFLVKQPEEPWF</td>\n",
       "      <td>cccccceeeccccccch</td>\n",
       "      <td>ehceceecehXce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QTEWKFADKAGKDLGFE</td>\n",
       "      <td>hhhhhhhhhhhhhcccc</td>\n",
       "      <td>cXhcXhXhche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VIKIAVPDGEKTLNAID</td>\n",
       "      <td>cceeeccchhhhhhhhh</td>\n",
       "      <td>hXcXecXeXh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SLAASGAKGFVICTPDP</td>\n",
       "      <td>hhhhccccccccccccc</td>\n",
       "      <td>ececchecXhecXh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KLGSAIVAKARGYDMKV</td>\n",
       "      <td>cccchhhhhhhhhcccc</td>\n",
       "      <td>chchceXece</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sequence             string         predict\n",
       "0  ENLKLGFLVKQPEEPWF  cccccceeeccccccch   ehceceecehXce\n",
       "1  QTEWKFADKAGKDLGFE  hhhhhhhhhhhhhcccc     cXhcXhXhche\n",
       "2  VIKIAVPDGEKTLNAID  cceeeccchhhhhhhhh      hXcXecXeXh\n",
       "3  SLAASGAKGFVICTPDP  hhhhccccccccccccc  ececchecXhecXh\n",
       "4  KLGSAIVAKARGYDMKV  cccchhhhhhhhhcccc      chchceXece"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
